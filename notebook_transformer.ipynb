{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross encoder Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarrabenyahia/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset snli (/Users/sarrabenyahia/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n",
      "Found cached dataset snli (/Users/sarrabenyahia/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n",
      "Found cached dataset snli (/Users/sarrabenyahia/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'cross-encoder/nli-roberta-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-roberta-base')\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = load_dataset(\"snli\", split='train')\n",
    "test_dataset = load_dataset(\"snli\", split='test')\n",
    "val_dataset = load_dataset(\"snli\", split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/sarrabenyahia/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-3fb44ea69c4768d5.arrow\n",
      "Loading cached processed dataset at /Users/sarrabenyahia/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-96d7b47e8248bd0b.arrow\n",
      "Loading cached processed dataset at /Users/sarrabenyahia/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-0183042e7957df42.arrow\n"
     ]
    }
   ],
   "source": [
    "# Filter out examples with label -1\n",
    "train_dataset_filtered = train_dataset.filter(\n",
    "    lambda example: example['label'] != -1)\n",
    "test_dataset_filtered = test_dataset.filter(\n",
    "    lambda example: example['label'] != -1)\n",
    "val_dataset_filtered = val_dataset.filter(\n",
    "    lambda example: example['label'] != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize, pad, and encode filtered sets\n",
    "train_encodings_filtered = tokenizer(train_dataset_filtered['premise'], train_dataset_filtered['hypothesis'],\n",
    "                                     padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "test_encodings_filtered = tokenizer(test_dataset_filtered['premise'], test_dataset_filtered['hypothesis'],\n",
    "                                    padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "val_encodings_filtered = tokenizer(val_dataset_filtered['premise'], val_dataset_filtered['hypothesis'],\n",
    "                                   padding=True, truncation=True, max_length=128, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_filtered_encoded = label_encoder.fit_transform(\n",
    "    train_dataset_filtered['label'])\n",
    "test_labels_filtered_encoded = label_encoder.transform(\n",
    "    test_dataset_filtered['label'])\n",
    "val_labels_filtered_encoded = label_encoder.transform(\n",
    "    val_dataset_filtered['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, ..., 2, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_filtered_encoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model on snli dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'contradiction', 1: 'entailment', 2: 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "#First we see the configuration of the id2label to map our labels. \n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained('cross-encoder/nli-roberta-base')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-roberta-base')\n",
    "\n",
    "print(config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Map ground truth labels to label names\n",
    "label_mapping = { 0: 'contradiction', 1: 'entailment', 2: 'neutral'}\n",
    "ground_truth_labels = [label_mapping[label] for label in test_labels_filtered_encoded]\n",
    "\n",
    "# Define batch size and number of workers for data loader\n",
    "batch_size = 16\n",
    "num_workers = 2\n",
    "\n",
    "# Define data loader\n",
    "test_loader = DataLoader(list(zip(test_dataset_filtered['premise'], test_dataset_filtered['hypothesis'], ground_truth_labels)),\n",
    "                         batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Move model to device and enable data parallelism if using multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f'Using {torch.cuda.device_count()} GPUs')\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in test_loader:\n",
    "        inputs = tokenizer(batch[0], batch[1], padding=True,\n",
    "                           truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        labels = batch[2]\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        predicted_labels = [label_mapping[prediction]\n",
    "                            for prediction in torch.argmax(outputs.logits, dim=1).tolist()]\n",
    "        total_correct += sum(1 for i in range(len(predicted_labels))\n",
    "                             if predicted_labels[i] == labels[i])\n",
    "        total_samples += len(predicted_labels)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = total_correct / total_samples\n",
    "print(f'Total samples: {total_samples}')\n",
    "print(f'Total correct: {total_correct}')\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
