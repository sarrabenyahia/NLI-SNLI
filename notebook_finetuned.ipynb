{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-encoder Transformer and fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarrabenyahia/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset snli (/Users/sarrabenyahia/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n",
      "Found cached dataset snli (/Users/sarrabenyahia/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n",
      "Found cached dataset snli (/Users/sarrabenyahia/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'cross-encoder/nli-roberta-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-roberta-base')\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = load_dataset(\"snli\", split='train')\n",
    "test_dataset = load_dataset(\"snli\", split='test')\n",
    "val_dataset = load_dataset(\"snli\", split='validation')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/sarrabenyahia/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-3fb44ea69c4768d5.arrow\n",
      "Loading cached processed dataset at /Users/sarrabenyahia/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-96d7b47e8248bd0b.arrow\n",
      "Loading cached processed dataset at /Users/sarrabenyahia/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-0183042e7957df42.arrow\n"
     ]
    }
   ],
   "source": [
    "# Filter out examples with label -1\n",
    "train_dataset_filtered = train_dataset.filter(\n",
    "    lambda example: example['label'] != -1)\n",
    "test_dataset_filtered = test_dataset.filter(\n",
    "    lambda example: example['label'] != -1)\n",
    "val_dataset_filtered = val_dataset.filter(\n",
    "    lambda example: example['label'] != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize, pad, and encode filtered sets\n",
    "train_encodings_filtered = tokenizer(\n",
    "    train_dataset_filtered['premise'], train_dataset_filtered['hypothesis'], padding=True, truncation=True, max_length=128)\n",
    "test_encodings_filtered = tokenizer(\n",
    "    test_dataset_filtered['premise'], test_dataset_filtered['hypothesis'], padding=True, truncation=True, max_length=128)\n",
    "val_encodings_filtered = tokenizer(\n",
    "    val_dataset_filtered['premise'], val_dataset_filtered['hypothesis'], padding=True, truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_filtered_encoded = label_encoder.fit_transform(\n",
    "    train_dataset_filtered['label'])\n",
    "test_labels_filtered_encoded = label_encoder.transform(\n",
    "    test_dataset_filtered['label'])\n",
    "val_labels_filtered_encoded = label_encoder.transform(\n",
    "    val_dataset_filtered['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs and labels to list of dictionaries\n",
    "train_inputs = {'input_ids': train_encodings_filtered['input_ids'],\n",
    "                'attention_mask': train_encodings_filtered['attention_mask']}\n",
    "train_data = []\n",
    "for i in range(len(train_encodings_filtered['input_ids'])):\n",
    "    train_data.append({key: torch.tensor(val[i])\n",
    "                      for key, val in train_inputs.items()})\n",
    "train_data = [{'input_ids': input['input_ids'], 'attention_mask': input['attention_mask'],\n",
    "               'labels': label} for input, label in zip(train_data, train_labels_filtered_encoded)]\n",
    "\n",
    "val_inputs = {'input_ids': val_encodings_filtered['input_ids'],\n",
    "              'attention_mask': val_encodings_filtered['attention_mask']}\n",
    "val_data = []\n",
    "for i in range(len(val_encodings_filtered['input_ids'])):\n",
    "    val_data.append({key: torch.tensor(val[i])\n",
    "                    for key, val in val_inputs.items()})\n",
    "val_data = [{'input_ids': input['input_ids'], 'attention_mask': input['attention_mask'],\n",
    "             'labels': label} for input, label in zip(val_data, val_labels_filtered_encoded)]\n",
    "\n",
    "test_inputs = {'input_ids': test_encodings_filtered['input_ids'],\n",
    "               'attention_mask': test_encodings_filtered['attention_mask']}\n",
    "test_data = []\n",
    "for i in range(len(test_encodings_filtered['input_ids'])):\n",
    "    test_data.append({key: torch.tensor(val[i])\n",
    "                     for key, val in test_inputs.items()})\n",
    "test_data = [{'input_ids': input['input_ids'], 'attention_mask': input['attention_mask'],\n",
    "              'labels': label} for input, label in zip(test_data, test_labels_filtered_encoded)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarrabenyahia/Library/Python/3.9/lib/python/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1250\n",
      "  Number of trainable parameters = 124647939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 500/1250 [18:07<27:23,  2.19s/it] ***** Running Evaluation *****\n",
      "  Num examples = 9842\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4862, 'learning_rate': 3.0002400192015362e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 40%|████      | 500/1250 [25:11<27:23,  2.19s/it]Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3121122717857361, 'eval_accuracy': 0.8962609225767121, 'eval_runtime': 424.5116, 'eval_samples_per_second': 23.184, 'eval_steps_per_second': 0.726, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      " 80%|████████  | 1000/1250 [42:24<05:41,  1.36s/it]   ***** Running Evaluation *****\n",
      "  Num examples = 9842\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3774, 'learning_rate': 1.0000800064005119e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|████████  | 1000/1250 [49:31<05:41,  1.36s/it]Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3315350413322449, 'eval_accuracy': 0.90367811420443, 'eval_runtime': 427.1176, 'eval_samples_per_second': 23.043, 'eval_steps_per_second': 0.721, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "100%|██████████| 1250/1250 [56:53<00:00,  2.15s/it]   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-500 (score: 0.3121122717857361).\n",
      "100%|██████████| 1250/1250 [56:54<00:00,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3414.2151, 'train_samples_per_second': 2.929, 'train_steps_per_second': 0.366, 'train_loss': 0.41317313232421876, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.41317313232421876, metrics={'train_runtime': 3414.2151, 'train_samples_per_second': 2.929, 'train_steps_per_second': 0.366, 'train_loss': 0.41317313232421876, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "\n",
    "# Train the model on a smaller sample of the training data\n",
    "random.seed(42)\n",
    "train_data_sample = random.sample(train_data, k=10000)\n",
    "\n",
    "# Fine-tune the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,  # increase the eval batch size\n",
    "    num_train_epochs=1,\n",
    "    metric_for_best_model='eval_loss',  # use eval_loss as the evaluation metric\n",
    "    warmup_steps=0.1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data_sample,\n",
    "    eval_dataset=val_data,\n",
    "    compute_metrics=lambda pred: {\"accuracy\": accuracy_score(\n",
    "        pred.label_ids, pred.predictions.argmax(axis=1))}\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9824\n",
      "  Batch size = 32\n",
      "100%|██████████| 307/307 [04:45<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32171836495399475, 'eval_accuracy': 0.8927117263843648, 'eval_runtime': 286.1031, 'eval_samples_per_second': 34.337, 'eval_steps_per_second': 1.073, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test dataset\n",
    "eval_result = trainer.evaluate(test_data)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss: 0.322\n",
      "eval_accuracy: 0.893\n",
      "eval_runtime: 286.103\n",
      "eval_samples_per_second: 34.337\n",
      "eval_steps_per_second: 1.073\n",
      "epoch: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Extract the metric names and values from the evaluation result\n",
    "metric_names = list(eval_result.keys())\n",
    "metric_values = list(eval_result.values())\n",
    "\n",
    "# Print the metric names and values\n",
    "for name, value in zip(metric_names, metric_values):\n",
    "    print('{}: {:.3f}'.format(name, value))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on the validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9842\n",
      "  Batch size = 32\n",
      "100%|██████████| 308/308 [07:07<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3121122717857361, 'eval_accuracy': 0.8962609225767121, 'eval_runtime': 429.2229, 'eval_samples_per_second': 22.93, 'eval_steps_per_second': 0.718, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on val dataset\n",
    "eval_result_val = trainer.evaluate(val_data)\n",
    "print(eval_result_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of the results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test with sentences outside of the SNLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file results_final_non_overfitted_accuracy_metrics/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"results_final_non_overfitted_accuracy_metrics\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"entailment\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file results_final_non_overfitted_accuracy_metrics/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at results_final_non_overfitted_accuracy_metrics.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "loading configuration file config.json from cache at /Users/sarrabenyahia/.cache/huggingface/hub/models--cross-encoder--nli-roberta-base/snapshots/1c9dadfb1d7bcaac49176fd3a5de914f6ae2bd42/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"cross-encoder/nli-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"entailment\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 1,\n",
      "    \"neutral\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /Users/sarrabenyahia/.cache/huggingface/hub/models--cross-encoder--nli-roberta-base/snapshots/1c9dadfb1d7bcaac49176fd3a5de914f6ae2bd42/vocab.json\n",
      "loading file merges.txt from cache at /Users/sarrabenyahia/.cache/huggingface/hub/models--cross-encoder--nli-roberta-base/snapshots/1c9dadfb1d7bcaac49176fd3a5de914f6ae2bd42/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/sarrabenyahia/.cache/huggingface/hub/models--cross-encoder--nli-roberta-base/snapshots/1c9dadfb1d7bcaac49176fd3a5de914f6ae2bd42/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/sarrabenyahia/.cache/huggingface/hub/models--cross-encoder--nli-roberta-base/snapshots/1c9dadfb1d7bcaac49176fd3a5de914f6ae2bd42/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/sarrabenyahia/.cache/huggingface/hub/models--cross-encoder--nli-roberta-base/snapshots/1c9dadfb1d7bcaac49176fd3a5de914f6ae2bd42/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"cross-encoder/nli-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"entailment\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 1,\n",
      "    \"neutral\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/sarrabenyahia/.cache/huggingface/hub/models--cross-encoder--nli-roberta-base/snapshots/1c9dadfb1d7bcaac49176fd3a5de914f6ae2bd42/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"cross-encoder/nli-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"entailment\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 1,\n",
      "    \"neutral\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted label for the pair of sentences is: 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'results_final_non_overfitted_accuracy_metrics')\n",
    "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-roberta-base')\n",
    "\n",
    "# Define the two example sentences to test\n",
    "sentence1 = \"The cat chased the mouse.\"\n",
    "sentence2 = \"The mouse was chased by the cat\"\n",
    "\n",
    "# Encode the two sentences\n",
    "encoding = tokenizer(sentence1, sentence2, padding=True,\n",
    "                     truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Feed the encoding into the model and get the predicted label\n",
    "outputs = model(**encoding)\n",
    "predicted_label = torch.argmax(outputs.logits).item()\n",
    "\n",
    "# Print the predicted label\n",
    "print(\"The predicted label for the pair of sentences is:\",\n",
    "      predicted_label)  # Should be entailment -> 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "output_dir = \"./results_final_non_overfitted_accuracy_metrics\"\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted label for the pair of sentences is: 2\n"
     ]
    }
   ],
   "source": [
    "# Define the two example sentences to test\n",
    "sentence1 = \"The man is playing soccer.\"\n",
    "sentence2 = \"The woman is cooking dinner.\"\n",
    "\n",
    "# Encode the two sentences\n",
    "encoding = tokenizer(sentence1, sentence2, padding=True,\n",
    "                     truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Feed the encoding into the model and get the predicted label\n",
    "outputs = model(**encoding)\n",
    "predicted_label = torch.argmax(outputs.logits).item()\n",
    "\n",
    "# Print the predicted label\n",
    "print(\"The predicted label for the pair of sentences is:\",\n",
    "      predicted_label)  # Should be contradiction -> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted label for the pair of sentences is: 1\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"The bird is flying in the sky.\"\n",
    "sentence2 = \"The sky is blue.\"\n",
    "\n",
    "encoding = tokenizer(sentence1, sentence2, padding=True,\n",
    "                     truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "outputs = model(**encoding)\n",
    "predicted_label = torch.argmax(outputs.logits).item()\n",
    "\n",
    "print(\"The predicted label for the pair of sentences is:\",\n",
    "      predicted_label)  # Should be neutral -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted label for the pair of sentences is: 2\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"The boy is jumping on the trampoline.\"\n",
    "sentence2 = \"The girl is reading a book.\"\n",
    "\n",
    "encoding = tokenizer(sentence1, sentence2, padding=True,\n",
    "                     truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "outputs = model(**encoding)\n",
    "predicted_label = torch.argmax(outputs.logits).item()\n",
    "\n",
    "# Print the predicted label\n",
    "print(\"The predicted label for the pair of sentences is:\",\n",
    "      predicted_label)  # Should be contradiction -> 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
